#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
å®Œå…¨ãªçš®è†šç—…å¤‰ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
BCCå¤§å¹…è¿½åŠ å¾Œã®å…¨ã‚¯ãƒ©ã‚¹å¯¾å¿œ
"""

import os
import json
import shutil
from pathlib import Path
from collections import defaultdict
import random

def analyze_organized_data():
    """organizedãƒ•ã‚©ãƒ«ãƒ€ã®ãƒ‡ãƒ¼ã‚¿åˆ†æ"""
    print("ğŸ“Š organizedãƒ•ã‚©ãƒ«ãƒ€ã®ãƒ‡ãƒ¼ã‚¿åˆ†æ")
    
    organized_dir = Path('organized')
    class_counts = {}
    
    for class_dir in organized_dir.iterdir():
        if class_dir.is_dir():
            # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ•°ãˆã‚‹
            image_files = list(class_dir.glob("*.jpg")) + list(class_dir.glob("*.JPG")) + \
                         list(class_dir.glob("*.png")) + list(class_dir.glob("*.bmp")) + \
                         list(class_dir.glob("*.tif"))
            
            class_counts[class_dir.name] = len(image_files)
            print(f"  {class_dir.name}: {len(image_files)}æš")
    
    total_images = sum(class_counts.values())
    print(f"\nç·ç”»åƒæ•°: {total_images}æš")
    
    # ä¸å‡è¡¡åº¦åˆ†æ
    max_count = max(class_counts.values())
    min_count = min(class_counts.values())
    print(f"ä¸å‡è¡¡æ¯”: {max_count}:{min_count} = {max_count/min_count:.1f}:1")
    
    return class_counts

def create_full_yolo_dataset():
    """å®Œå…¨ãªYOLOãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ"""
    print("\nğŸ—ï¸ å®Œå…¨ãªYOLOãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆé–‹å§‹")
    
    # ã‚¯ãƒ©ã‚¹ãƒãƒƒãƒ”ãƒ³ã‚°
    class_mapping = {
        'ADM': 0,
        'Ephelis': 1, 
        'Melasma': 2,
        'Solar lentigo': 3,
        'Nevus': 4,
        'Basal cell carcinoma': 5,
        'Seborrheic keratosis': 6,
        'Malignant melanoma': 7
    }
    
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæº–å‚™
    output_dir = Path('yolo_dataset_full')
    if output_dir.exists():
        shutil.rmtree(output_dir)
    
    # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ä½œæˆ
    for split in ['train', 'valid', 'test']:
        (output_dir / split / 'images').mkdir(parents=True, exist_ok=True)
        (output_dir / split / 'labels').mkdir(parents=True, exist_ok=True)
    
    organized_dir = Path('organized')
    all_annotations = []
    total_copied = 0
    
    print("\nğŸ“ å„ã‚¯ãƒ©ã‚¹ã®å‡¦ç†:")
    
    for class_name, class_id in class_mapping.items():
        class_dir = organized_dir / class_name
        
        if not class_dir.exists():
            print(f"âš ï¸ {class_name} ãƒ•ã‚©ãƒ«ãƒ€ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            continue
        
        # ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«å–å¾—
        image_files = []
        for ext in ['*.jpg', '*.JPG', '*.png', '*.bmp', '*.tif']:
            image_files.extend(list(class_dir.glob(ext)))
        
        if not image_files:
            print(f"âš ï¸ {class_name}: ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ãªã—")
            continue
        
        print(f"  {class_name}: {len(image_files)}æš â†’ å‡¦ç†ä¸­...")
        
        # ãƒ‡ãƒ¼ã‚¿åˆ†å‰²ï¼ˆtrain:70%, valid:20%, test:10%ï¼‰
        random.shuffle(image_files)
        train_count = int(len(image_files) * 0.7)
        valid_count = int(len(image_files) * 0.2)
        
        splits = {
            'train': image_files[:train_count],
            'valid': image_files[train_count:train_count + valid_count],
            'test': image_files[train_count + valid_count:]
        }
        
        for split, files in splits.items():
            for i, img_path in enumerate(files):
                # ç”»åƒã‚³ãƒ”ãƒ¼
                new_name = f"{class_name}_{i:04d}_{split}.jpg"
                dst_img = output_dir / split / 'images' / new_name
                
                try:
                    shutil.copy2(img_path, dst_img)
                    
                    # ä»®ã®ãƒ©ãƒ™ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆï¼ˆå¾Œã§ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³è¦ï¼‰
                    label_path = output_dir / split / 'labels' / f"{new_name.replace('.jpg', '.txt')}"
                    with open(label_path, 'w') as f:
                        # ä»®ã®ä¸­å¤®ä½ç½®ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³
                        f.write(f"{class_id} 0.5 0.5 0.3 0.3\n")
                    
                    # ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³è¨˜éŒ²
                    all_annotations.append({
                        'image_path': str(dst_img),
                        'label_path': str(label_path),
                        'class_name': class_name,
                        'class_id': class_id,
                        'split': split,
                        'original_path': str(img_path),
                        'needs_annotation': True
                    })
                    
                    total_copied += 1
                    
                except Exception as e:
                    print(f"    ã‚¨ãƒ©ãƒ¼: {img_path} â†’ {e}")
        
        print(f"    å®Œäº†: train={len(splits['train'])}, valid={len(splits['valid'])}, test={len(splits['test'])}")
    
    print(f"\nâœ… ç·ã‚³ãƒ”ãƒ¼æ•°: {total_copied}æš")
    
    # ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³æƒ…å ±ä¿å­˜
    with open('full_dataset_annotations.json', 'w', encoding='utf-8') as f:
        json.dump(all_annotations, f, ensure_ascii=False, indent=2)
    
    # dataset.yamlä½œæˆ
    yaml_content = f"""path: {output_dir.absolute()}
train: train/images
val: valid/images
test: test/images

nc: 8
names: ['ADM', 'Ephelis', 'Melasma', 'Solar lentigo', 'Nevus', 'Basal cell carcinoma', 'Seborrheic keratosis', 'Malignant melanoma']
"""
    
    with open(output_dir / 'dataset.yaml', 'w') as f:
        f.write(yaml_content)
    
    # çµ±è¨ˆæƒ…å ±
    print("\nğŸ“ˆ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆçµ±è¨ˆ:")
    for split in ['train', 'valid', 'test']:
        img_count = len(list((output_dir / split / 'images').glob('*.jpg')))
        print(f"  {split}: {img_count}æš")
    
    return output_dir

def create_annotation_plan():
    """ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³è¨ˆç”»ä½œæˆ"""
    print("\nğŸ“ ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³è¨ˆç”»")
    
    with open('full_dataset_annotations.json', 'r', encoding='utf-8') as f:
        annotations = json.load(f)
    
    # ã‚¯ãƒ©ã‚¹åˆ¥çµ±è¨ˆ
    class_stats = defaultdict(int)
    for ann in annotations:
        class_stats[ann['class_name']] += 1
    
    print("ã‚¯ãƒ©ã‚¹åˆ¥ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å¿…è¦æ•°:")
    total_needed = 0
    for class_name, count in sorted(class_stats.items()):
        print(f"  {class_name}: {count}æš")
        total_needed += count
    
    print(f"\nç·ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³å¿…è¦æ•°: {total_needed}æš")
    print("\nğŸ’¡ æ¨å¥¨ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³æ‰‹é †:")
    print("1. å°‘æ•°ã‚¯ãƒ©ã‚¹ã‹ã‚‰é–‹å§‹ï¼ˆBasal cell carcinomaç­‰ï¼‰")
    print("2. å„ã‚¯ãƒ©ã‚¹50-100æšç¨‹åº¦ã‚’å…ˆã«ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³")
    print("3. åˆæœŸå­¦ç¿’ã§æ¤œè¨¼")
    print("4. æ®µéšçš„ã«å…¨ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³")
    
    # å„ªå…ˆåº¦ä»˜ãã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³è¨ˆç”»
    priority_plan = {
        'Phase 1 (é«˜å„ªå…ˆåº¦)': ['Basal cell carcinoma', 'Malignant melanoma'],
        'Phase 2 (ä¸­å„ªå…ˆåº¦)': ['ADM', 'Solar lentigo', 'Seborrheic keratosis'],
        'Phase 3 (æ¨™æº–)': ['Ephelis', 'Nevus', 'Melasma']
    }
    
    print("\nğŸ¯ æ®µéšçš„ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³è¨ˆç”»:")
    for phase, classes in priority_plan.items():
        print(f"\n{phase}:")
        phase_total = sum(class_stats[cls] for cls in classes if cls in class_stats)
        for cls in classes:
            if cls in class_stats:
                print(f"  - {cls}: {class_stats[cls]}æš")
        print(f"  å°è¨ˆ: {phase_total}æš")

def main():
    print("ğŸ”¬ å®Œå…¨ãªçš®è†šç—…å¤‰ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ")
    print("=" * 60)
    print("BCCå¤§å¹…è¿½åŠ å¾Œã®å…¨ãƒ‡ãƒ¼ã‚¿å¯¾å¿œ")
    
    # 1. ãƒ‡ãƒ¼ã‚¿åˆ†æ
    class_counts = analyze_organized_data()
    
    # 2. YOLOãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
    dataset_dir = create_full_yolo_dataset()
    
    # 3. ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³è¨ˆç”»
    create_annotation_plan()
    
    print(f"\nğŸ‰ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆå®Œäº†!")
    print(f"å‡ºåŠ›å…ˆ: {dataset_dir}")
    print("\næ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
    print("1. python yolo_annotator.py ã§ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³é–‹å§‹")
    print("2. æ®µéšçš„ã«ã‚¢ãƒãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œ")
    print("3. é©åˆ‡ãªã‚¿ã‚¤ãƒŸãƒ³ã‚°ã§å­¦ç¿’é–‹å§‹")

if __name__ == "__main__":
    random.seed(42)  # å†ç¾æ€§ã®ãŸã‚
    main()