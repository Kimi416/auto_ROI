#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Hard Negative Mining: èª¤æ¤œå‡ºãƒ‡ãƒ¼ã‚¿åé›†å™¨
ç¾åœ¨ã®ãƒ¢ãƒ‡ãƒ«ã§ä½é–¾å€¤æ¤œå‡ºã‚’è¡Œã„ã€èª¤æ¤œå‡ºã‚’ç‰¹å®šãƒ»åé›†ã™ã‚‹
"""

import cv2
import numpy as np
import json
from pathlib import Path
import shutil
from ultralytics import YOLO

class FalsePositiveCollector:
    def __init__(self):
        self.model_path = "fast_lesion_training/training_runs/fast_lesion_20250806_095404/weights/best.pt"
        self.model = YOLO(self.model_path)
        
        # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        self.negatives_dir = Path("datasets/negatives")
        self.negatives_images_dir = self.negatives_dir / "images"
        self.negatives_labels_dir = self.negatives_dir / "labels"
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        self.negatives_images_dir.mkdir(parents=True, exist_ok=True)
        self.negatives_labels_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"ğŸ¯ Hard Negative Mining ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº†")
        print(f"ğŸ“ å‡ºåŠ›å…ˆ: {self.negatives_dir}")
    
    def collect_from_validation_set(self):
        """æ¤œè¨¼ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰èª¤æ¤œå‡ºã‚’åé›†"""
        print(f"\nğŸ” æ¤œè¨¼ã‚»ãƒƒãƒˆã‹ã‚‰ã®èª¤æ¤œå‡ºåé›†é–‹å§‹")
        print("=" * 60)
        
        # æ—¢å­˜ã®æ¤œè¨¼ç”¨ç”»åƒã‚’å¯¾è±¡ã«ã™ã‚‹
        validation_sources = [
            "test_preview.jpg",
            "test1.jpeg", 
            "test2.jpeg"
        ]
        
        # organized_advanced_maskedã‹ã‚‰ä¸€éƒ¨ç”»åƒã‚‚æ¤œè¨¼ç”¨ã¨ã—ã¦ä½¿ç”¨
        masked_dir = Path("organized_advanced_masked")
        additional_samples = []
        
        for class_dir in masked_dir.glob("*"):
            if class_dir.is_dir():
                # å„ã‚¯ãƒ©ã‚¹ã‹ã‚‰5æšãšã¤ã‚µãƒ³ãƒ—ãƒ«
                sample_images = list(class_dir.glob("*.jpg"))[:5]
                additional_samples.extend(sample_images)
        
        print(f"ğŸ“Š æ¤œè¨¼å¯¾è±¡: {len(validation_sources)}å€‹ã®æ—¢å­˜ãƒ†ã‚¹ãƒˆç”»åƒ + {len(additional_samples)}å€‹ã®è¿½åŠ ã‚µãƒ³ãƒ—ãƒ«")
        
        all_false_positives = []
        
        # æ—¢å­˜ãƒ†ã‚¹ãƒˆç”»åƒã§ã®æ¤œå‡º
        for img_path in validation_sources:
            if Path(img_path).exists():
                fps = self.detect_false_positives(img_path, is_known_negative=True)
                all_false_positives.extend(fps)
        
        # è¿½åŠ ã‚µãƒ³ãƒ—ãƒ«ã§ã®æ¤œå‡ºï¼ˆä½ä¿¡é ¼åº¦ã§ï¼‰
        for img_path in additional_samples[:20]:  # æœ€åˆã®20æšã®ã¿
            fps = self.detect_false_positives(str(img_path), is_known_negative=False)
            all_false_positives.extend(fps)
        
        print(f"\nğŸ“ˆ èª¤æ¤œå‡ºåé›†çµæœ: {len(all_false_positives)}å€‹")
        return all_false_positives
    
    def detect_false_positives(self, image_path, is_known_negative=False):
        """å˜ä¸€ç”»åƒã§ã®èª¤æ¤œå‡ºæ¤œå‡º"""
        print(f"ğŸ” {Path(image_path).name} ã‚’åˆ†æä¸­...")
        
        # ä½ä¿¡é ¼åº¦ã§æ¤œå‡ºå®Ÿè¡Œ
        confidence_thresholds = [0.01, 0.05, 0.1, 0.2]
        
        detections = []
        for conf in confidence_thresholds:
            results = self.model(image_path, conf=conf, verbose=False)
            
            for result in results:
                if result.boxes is not None:
                    for box in result.boxes:
                        cls_id = int(box.cls[0])
                        confidence = float(box.conf[0])
                        x1, y1, x2, y2 = map(int, box.xyxy[0])
                        
                        detections.append({
                            'image_path': image_path,
                            'confidence': confidence,
                            'threshold': conf,
                            'bbox': [x1, y1, x2, y2],
                            'class_id': cls_id,
                            'is_known_negative': is_known_negative
                        })
            
            if detections:
                break  # æœ€åˆã«æ¤œå‡ºã•ã‚ŒãŸthresholdã§åœæ­¢
        
        if detections:
            print(f"  âœ… {len(detections)}å€‹ã®æ¤œå‡ºï¼ˆä¿¡é ¼åº¦{detections[0]['threshold']}ï¼‰")
        else:
            print(f"  âŒ æ¤œå‡ºãªã—")
            
        # known negativeã®å ´åˆã¯å…¨ã¦èª¤æ¤œå‡ºã¨ã—ã¦æ‰±ã†
        if is_known_negative:
            return detections
        
        # é€šå¸¸ã®å ´åˆã¯æ‰‹å‹•ç¢ºèªãŒå¿…è¦ã ãŒã€ä»Šå›ã¯ä½ä¿¡é ¼åº¦ã®ã‚‚ã®ã‚’ç–‘ã‚ã—ã„ã¨ã™ã‚‹
        suspicious = [d for d in detections if d['confidence'] < 0.3]
        return suspicious
    
    def create_negative_dataset(self, false_positives):
        """èª¤æ¤œå‡ºãƒ‡ãƒ¼ã‚¿ã‹ã‚‰ãƒã‚¬ãƒ†ã‚£ãƒ–ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆ"""
        print(f"\nğŸ“¦ ãƒã‚¬ãƒ†ã‚£ãƒ–ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆé–‹å§‹")
        print("=" * 60)
        
        # ç”»åƒã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        fp_by_image = {}
        for fp in false_positives:
            img_path = fp['image_path']
            if img_path not in fp_by_image:
                fp_by_image[img_path] = []
            fp_by_image[img_path].append(fp)
        
        created_count = 0
        for img_path, detections in fp_by_image.items():
            if self.process_negative_image(img_path, detections):
                created_count += 1
        
        print(f"ğŸ“Š ä½œæˆå®Œäº†: {created_count}å€‹ã®ãƒã‚¬ãƒ†ã‚£ãƒ–ã‚µãƒ³ãƒ—ãƒ«")
        
        # dataset.yamlã‚’æ›´æ–°
        self.update_dataset_yaml()
        
        return created_count
    
    def process_negative_image(self, image_path, detections):
        """å˜ä¸€ç”»åƒã‚’ãƒã‚¬ãƒ†ã‚£ãƒ–ã‚µãƒ³ãƒ—ãƒ«ã¨ã—ã¦å‡¦ç†"""
        src_path = Path(image_path)
        
        if not src_path.exists():
            return False
        
        # æ–°ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ç”Ÿæˆ
        neg_filename = f"neg_{src_path.stem}_{len(detections)}det{src_path.suffix}"
        
        # ç”»åƒã‚’ã‚³ãƒ”ãƒ¼
        dst_image_path = self.negatives_images_dir / neg_filename
        shutil.copy2(src_path, dst_image_path)
        
        # ç©ºã®ãƒ©ãƒ™ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆï¼ˆé‡è¦ï¼ï¼‰
        label_filename = neg_filename.replace(src_path.suffix, '.txt')
        dst_label_path = self.negatives_labels_dir / label_filename
        dst_label_path.write_text("")  # ç©ºãƒ•ã‚¡ã‚¤ãƒ«
        
        print(f"  ğŸ“‹ {neg_filename} ã‚’ä½œæˆï¼ˆ{len(detections)}å€‹ã®èª¤æ¤œå‡ºã‚ã‚Šï¼‰")
        
        return True
    
    def update_dataset_yaml(self):
        """dataset.yamlã‚’æ›´æ–°ã—ã¦negativesã‚’å«ã‚ã‚‹"""
        yaml_path = Path("lesion_detection.yaml")
        
        if not yaml_path.exists():
            # æ–°è¦ä½œæˆ
            yaml_content = f"""# Hard Negative Miningå¯¾å¿œãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆè¨­å®š
path: {Path.cwd()}
train:
  - fast_lesion_training/yolo_dataset/train/images
  - datasets/negatives/images
val: fast_lesion_training/yolo_dataset/val/images
test: fast_lesion_training/yolo_dataset/test/images

# ã‚¯ãƒ©ã‚¹
nc: 6
names:
  0: ACK
  1: BCC  
  2: MEL
  3: NEV
  4: SCC
  5: SEK
"""
        else:
            # æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ›´æ–°
            with open(yaml_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # trainã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ›´æ–°
            if 'datasets/negatives/images' not in content:
                yaml_content = content.replace(
                    'train: fast_lesion_training/yolo_dataset/train/images',
                    '''train:
  - fast_lesion_training/yolo_dataset/train/images
  - datasets/negatives/images'''
                )
            else:
                yaml_content = content
        
        with open(yaml_path, 'w', encoding='utf-8') as f:
            f.write(yaml_content)
        
        print(f"âœ… dataset.yaml ã‚’æ›´æ–°å®Œäº†")
    
    def generate_collection_report(self, false_positives, created_count):
        """åé›†ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        report = {
            'timestamp': str(Path().resolve()),
            'model_used': self.model_path,
            'total_false_positives': len(false_positives),
            'negative_samples_created': created_count,
            'confidence_distribution': {},
            'class_distribution': {}
        }
        
        # ä¿¡é ¼åº¦åˆ†å¸ƒ
        conf_ranges = [(0, 0.1), (0.1, 0.2), (0.2, 0.3), (0.3, 1.0)]
        for low, high in conf_ranges:
            count = len([fp for fp in false_positives if low <= fp['confidence'] < high])
            report['confidence_distribution'][f"{low}-{high}"] = count
        
        # ã‚¯ãƒ©ã‚¹åˆ†å¸ƒ
        class_names = ['ACK', 'BCC', 'MEL', 'NEV', 'SCC', 'SEK']
        for cls_id in range(6):
            count = len([fp for fp in false_positives if fp['class_id'] == cls_id])
            report['class_distribution'][class_names[cls_id]] = count
        
        # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜
        with open('false_positive_collection_report.json', 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“Š åé›†ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜: false_positive_collection_report.json")
        
        return report

def main():
    collector = FalsePositiveCollector()
    
    print("ğŸš€ Hard Negative Mining é–‹å§‹")
    print("=" * 80)
    
    # Step 1: èª¤æ¤œå‡ºãƒ‡ãƒ¼ã‚¿åé›†
    false_positives = collector.collect_from_validation_set()
    
    if not false_positives:
        print("\nâŒ èª¤æ¤œå‡ºãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        print("ğŸ’¡ ã‚ˆã‚Šä½ã„ä¿¡é ¼åº¦ã§ã®æ¤œå‡ºã‚„ã€ã‚ˆã‚Šå¤šãã®æ¤œè¨¼ç”»åƒãŒå¿…è¦ã§ã™")
        return
    
    # Step 2: ãƒã‚¬ãƒ†ã‚£ãƒ–ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
    created_count = collector.create_negative_dataset(false_positives)
    
    # Step 3: ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
    report = collector.generate_collection_report(false_positives, created_count)
    
    print(f"\nğŸ‰ Hard Negative Mining åé›†å®Œäº†!")
    print(f"ğŸ“Š åé›†ã‚µãƒãƒªãƒ¼:")
    print(f"  â€¢ èª¤æ¤œå‡ºç·æ•°: {len(false_positives)}")
    print(f"  â€¢ ãƒã‚¬ãƒ†ã‚£ãƒ–ã‚µãƒ³ãƒ—ãƒ«: {created_count}å€‹")
    print(f"  â€¢ ä¿¡é ¼åº¦åˆ†å¸ƒ: {report['confidence_distribution']}")
    print(f"  â€¢ ã‚¯ãƒ©ã‚¹åˆ†å¸ƒ: {report['class_distribution']}")
    
    print(f"\nğŸ”„ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
    print(f"  1. Hard Negativeãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè¡Œ")
    print(f"  2. python3 hard_negative_trainer.py")

if __name__ == "__main__":
    main()