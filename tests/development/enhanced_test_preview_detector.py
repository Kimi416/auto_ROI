#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
test_preview.jpgå°‚ç”¨ã®å¼·åŒ–ç—…å¤‰æ¤œå‡ºå™¨
è¤‡æ•°ã®æ‰‹æ³•ã‚’çµ„ã¿åˆã‚ã›ã¦æ¤œå‡ºç²¾åº¦ã‚’å‘ä¸Š
"""

import cv2
import numpy as np
import matplotlib.pyplot as plt
from ultralytics import YOLO

class EnhancedTestPreviewDetector:
    def __init__(self):
        self.model_path = "fast_lesion_training/training_runs/fast_lesion_20250806_095404/weights/best.pt"
        self.model = YOLO(self.model_path)
        
        # ã‚¯ãƒ©ã‚¹å®šç¾©
        self.pad_classes = ['ACK', 'BCC', 'MEL', 'NEV', 'SCC', 'SEK']
        self.class_names_jp = {
            'ACK': 'æ—¥å…‰è§’åŒ–ç—‡',
            'BCC': 'åŸºåº•ç´°èƒç™Œ', 
            'MEL': 'æ‚ªæ€§é»’è‰²è…«',
            'NEV': 'è‰²ç´ æ€§æ¯æ–‘',
            'SCC': 'æœ‰æ£˜ç´°èƒç™Œ',
            'SEK': 'è„‚æ¼æ€§è§’åŒ–ç—‡'
        }
        
        self.colors_bgr = {
            'ACK': (0, 255, 0),      # ç·‘
            'BCC': (0, 0, 255),      # èµ¤
            'MEL': (255, 0, 255),    # ãƒã‚¼ãƒ³ã‚¿
            'NEV': (255, 255, 0),    # ã‚·ã‚¢ãƒ³
            'SCC': (0, 165, 255),    # ã‚ªãƒ¬ãƒ³ã‚¸
            'SEK': (128, 0, 128)     # ç´«
        }
    
    def preprocess_image_variants(self, image_path):
        """ç”»åƒã®è¤‡æ•°ã®ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ä½œæˆ"""
        original = cv2.imread(image_path)
        variants = []
        
        # 1. ã‚ªãƒªã‚¸ãƒŠãƒ«
        variants.append(("original", original))
        
        # 2. ã‚³ãƒ³ãƒˆãƒ©ã‚¹ãƒˆå¼·åŒ–
        enhanced = cv2.convertScaleAbs(original, alpha=1.3, beta=10)
        variants.append(("enhanced_contrast", enhanced))
        
        # 3. ãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ å¹³å¦åŒ–
        lab = cv2.cvtColor(original, cv2.COLOR_BGR2LAB)
        lab[:,:,0] = cv2.equalizeHist(lab[:,:,0])
        equalized = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)
        variants.append(("histogram_equalized", equalized))
        
        # 4. ã‚·ãƒ£ãƒ¼ãƒ—åŒ–
        kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])
        sharpened = cv2.filter2D(original, -1, kernel)
        variants.append(("sharpened", sharpened))
        
        # 5. ã‚¬ãƒ³ãƒè£œæ­£
        gamma = 0.7
        invGamma = 1.0 / gamma
        table = np.array([((i / 255.0) ** invGamma) * 255 for i in np.arange(0, 256)]).astype("uint8")
        gamma_corrected = cv2.LUT(original, table)
        variants.append(("gamma_corrected", gamma_corrected))
        
        return variants
    
    def detect_on_variant(self, variant_name, image, conf_thresholds):
        """ç‰¹å®šã®ç”»åƒãƒãƒªã‚¢ãƒ³ãƒˆã§æ¤œå‡ºå®Ÿè¡Œ"""
        print(f"ğŸ” {variant_name}ã§æ¤œå‡ºå®Ÿè¡Œä¸­...")
        
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        temp_path = f"temp_{variant_name}.jpg"
        cv2.imwrite(temp_path, image)
        
        best_detections = []
        best_conf = None
        
        for conf in conf_thresholds:
            results = self.model(temp_path, conf=conf, verbose=False)
            
            detections = []
            for result in results:
                if result.boxes is not None:
                    for box in result.boxes:
                        cls_id = int(box.cls[0])
                        confidence = float(box.conf[0])
                        class_name = self.pad_classes[cls_id]
                        x1, y1, x2, y2 = map(int, box.xyxy[0])
                        
                        detections.append({
                            'class': class_name,
                            'class_jp': self.class_names_jp[class_name],
                            'confidence': confidence,
                            'bbox': [x1, y1, x2, y2],
                            'variant': variant_name,
                            'threshold': conf
                        })
            
            if detections:
                best_detections = detections
                best_conf = conf
                print(f"  âœ… {variant_name}: {len(detections)}å€‹ã®ç—…å¤‰ã‚’æ¤œå‡ºï¼ˆä¿¡é ¼åº¦: {conf}ï¼‰")
                break
            else:
                print(f"  âŒ {variant_name}: ä¿¡é ¼åº¦{conf}ã§æ¤œå‡ºãªã—")
        
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
        import os
        if os.path.exists(temp_path):
            os.remove(temp_path)
            
        return best_detections
    
    def multi_scale_detection(self, image_path):
        """ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«æ¤œå‡º"""
        print("ğŸ¯ ãƒãƒ«ãƒã‚¹ã‚±ãƒ¼ãƒ«æ¤œå‡ºé–‹å§‹")
        print("=" * 60)
        
        # è¶…ä½ä¿¡é ¼åº¦ã§ã®æ¤œå‡º
        ultra_low_conf = [0.001, 0.005, 0.01, 0.02, 0.05, 0.1]
        
        # è¤‡æ•°ãƒãƒªã‚¢ãƒ³ãƒˆã§æ¤œå‡º
        variants = self.preprocess_image_variants(image_path)
        all_detections = []
        
        for variant_name, image in variants:
            detections = self.detect_on_variant(variant_name, image, ultra_low_conf)
            all_detections.extend(detections)
        
        if not all_detections:
            print("\nâŒ ã™ã¹ã¦ã®ãƒãƒªã‚¢ãƒ³ãƒˆã§ç—…å¤‰ãŒæ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
            
            # æœ€å¾Œã®æ‰‹æ®µï¼šç”»åƒè§£æã«ã‚ˆã‚‹å€™è£œé ˜åŸŸæ¤œå‡º
            print("\nğŸ”¬ ç”»åƒè§£æã«ã‚ˆã‚‹å€™è£œé ˜åŸŸæ¤œå‡ºã‚’è©¦è¡Œ...")
            candidates = self.analyze_suspicious_regions(image_path)
            return candidates
        
        # æ¤œå‡ºçµæœã®çµ±åˆ
        print(f"\nğŸ“Š æ¤œå‡ºçµæœçµ±åˆ: {len(all_detections)}å€‹ã®å€™è£œ")
        merged_detections = self.merge_similar_detections(all_detections)
        
        return merged_detections
    
    def analyze_suspicious_regions(self, image_path):
        """ç”»åƒè§£æã«ã‚ˆã‚‹ç–‘ã‚ã—ã„é ˜åŸŸã®æ¤œå‡º"""
        original = cv2.imread(image_path)
        gray = cv2.cvtColor(original, cv2.COLOR_BGR2GRAY)
        
        suspicious_regions = []
        
        # 1. è‰²ã®å¤‰åŒ–ãŒå¤§ãã„é ˜åŸŸã‚’æ¤œå‡º
        # HSVã«å¤‰æ›
        hsv = cv2.cvtColor(original, cv2.COLOR_BGR2HSV)
        
        # èŒ¶è‰²ãƒ»é»’è‰²ç³»ã®é ˜åŸŸã‚’æ¤œå‡ºï¼ˆä¸€èˆ¬çš„ãªçš®è†šç—…å¤‰ã®è‰²ï¼‰
        lower_brown = np.array([10, 50, 20])
        upper_brown = np.array([20, 255, 200])
        brown_mask = cv2.inRange(hsv, lower_brown, upper_brown)
        
        lower_dark = np.array([0, 0, 0])
        upper_dark = np.array([180, 255, 80])
        dark_mask = cv2.inRange(hsv, lower_dark, upper_dark)
        
        combined_mask = cv2.bitwise_or(brown_mask, dark_mask)
        
        # è¼ªéƒ­æ¤œå‡º
        contours, _ = cv2.findContours(combined_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
        
        print(f"ğŸ” {len(contours)}å€‹ã®å€™è£œé ˜åŸŸã‚’ç™ºè¦‹")
        
        for i, contour in enumerate(contours):
            area = cv2.contourArea(contour)
            if area > 100:  # æœ€å°é¢ç©ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼
                x, y, w, h = cv2.boundingRect(contour)
                
                # ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”ãƒã‚§ãƒƒã‚¯ï¼ˆæ¥µç«¯ã«ç´°é•·ã„é ˜åŸŸã‚’é™¤å¤–ï¼‰
                aspect_ratio = w / h
                if 0.3 < aspect_ratio < 3.0:
                    suspicious_regions.append({
                        'class': 'UNKNOWN',
                        'class_jp': 'ç–‘ã‚ã—ã„é ˜åŸŸ',
                        'confidence': 0.5,  # å›ºå®šå€¤
                        'bbox': [x, y, x+w, y+h],
                        'variant': 'image_analysis',
                        'threshold': 'analysis',
                        'area': area
                    })
        
        print(f"ğŸ“ {len(suspicious_regions)}å€‹ã®æœ‰åŠ¹ãªç–‘ã‚ã—ã„é ˜åŸŸã‚’ç‰¹å®š")
        return suspicious_regions
    
    def merge_similar_detections(self, detections):
        """é¡ä¼¼ã™ã‚‹æ¤œå‡ºçµæœã‚’ãƒãƒ¼ã‚¸"""
        if not detections:
            return []
        
        # ä¿¡é ¼åº¦ã§ã‚½ãƒ¼ãƒˆ
        detections.sort(key=lambda x: x['confidence'], reverse=True)
        
        merged = []
        for det in detections:
            # æ—¢å­˜ã®æ¤œå‡ºã¨é‡è¤‡ãƒã‚§ãƒƒã‚¯
            is_duplicate = False
            for existing in merged:
                if self.calculate_iou(det['bbox'], existing['bbox']) > 0.3:
                    # é‡è¤‡ã—ã¦ã„ã‚‹å ´åˆã€ã‚ˆã‚Šé«˜ã„ä¿¡é ¼åº¦ã‚’ä¿æŒ
                    if det['confidence'] > existing['confidence']:
                        merged.remove(existing)
                        merged.append(det)
                    is_duplicate = True
                    break
            
            if not is_duplicate:
                merged.append(det)
        
        return merged[:10]  # æœ€å¤§10å€‹ã¾ã§
    
    def calculate_iou(self, box1, box2):
        """IoUï¼ˆIntersection over Unionï¼‰ã‚’è¨ˆç®—"""
        x1_1, y1_1, x2_1, y2_1 = box1
        x1_2, y1_2, x2_2, y2_2 = box2
        
        # äº¤å·®é ˜åŸŸ
        x1_i = max(x1_1, x1_2)
        y1_i = max(y1_1, y1_2)
        x2_i = min(x2_1, x2_2)
        y2_i = min(y2_1, y2_2)
        
        if x2_i <= x1_i or y2_i <= y1_i:
            return 0.0
        
        intersection = (x2_i - x1_i) * (y2_i - y1_i)
        area1 = (x2_1 - x1_1) * (y2_1 - y1_1)
        area2 = (x2_2 - x1_2) * (y2_2 - y1_2)
        union = area1 + area2 - intersection
        
        return intersection / union if union > 0 else 0.0
    
    def visualize_results(self, image_path, detections):
        """æ¤œå‡ºçµæœã®å¯è¦–åŒ–"""
        original = cv2.imread(image_path)
        annotated = original.copy()
        
        print(f"\nğŸ“Š æœ€çµ‚æ¤œå‡ºçµæœ: {len(detections)}å€‹")
        
        for i, det in enumerate(detections, 1):
            x1, y1, x2, y2 = det['bbox']
            class_name = det['class']
            class_jp = det['class_jp']
            conf = det['confidence']
            variant = det['variant']
            threshold = det['threshold']
            
            color = self.colors_bgr.get(class_name, (255, 255, 255))
            
            print(f"  {i}. {class_jp} ({class_name})")
            print(f"     ä¿¡é ¼åº¦: {conf:.3f}")
            print(f"     æ¤œå‡ºæ–¹æ³•: {variant} (é–¾å€¤: {threshold})")
            print(f"     ä½ç½®: ({x1}, {y1}) - ({x2}, {y2})")
            print(f"     ã‚µã‚¤ã‚º: {x2-x1}px Ã— {y2-y1}px")
            print()
            
            # ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹æç”»
            cv2.rectangle(annotated, (x1, y1), (x2, y2), color, 6)
            
            # ãƒ©ãƒ™ãƒ«æç”»
            label = f"{i}. {class_name} {conf:.3f}"
            label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 1.0, 3)[0]
            
            # ãƒ©ãƒ™ãƒ«ä½ç½®èª¿æ•´
            label_y = y1 - 20
            if label_y < 50:
                label_y = y2 + 50
                
            # ãƒ©ãƒ™ãƒ«èƒŒæ™¯
            cv2.rectangle(annotated, 
                        (x1, label_y - label_size[1] - 15), 
                        (x1 + label_size[0] + 15, label_y + 10), 
                        color, -1)
            
            # ãƒ©ãƒ™ãƒ«ãƒ†ã‚­ã‚¹ãƒˆ
            cv2.putText(annotated, label, (x1 + 8, label_y - 8), 
                       cv2.FONT_HERSHEY_SIMPLEX, 1.0, (255, 255, 255), 3)
            
            # ä¸­å¤®ã«ç•ªå·
            center_x = (x1 + x2) // 2
            center_y = (y1 + y2) // 2
            cv2.circle(annotated, (center_x, center_y), 30, color, -1)
            cv2.putText(annotated, str(i), (center_x-15, center_y+15), 
                       cv2.FONT_HERSHEY_SIMPLEX, 2.0, (255, 255, 255), 4)
        
        # çµæœä¿å­˜
        output_path = "enhanced_test_preview_detection_result.jpg"
        cv2.imwrite(output_path, annotated)
        print(f"ğŸ’¾ å¼·åŒ–æ¤œå‡ºçµæœä¿å­˜: {output_path}")
        
        # æ¯”è¼ƒå¯è¦–åŒ–
        self.create_comparison_visualization(original, annotated, detections)
        
        return detections
    
    def create_comparison_visualization(self, original, annotated, detections):
        """æ¯”è¼ƒå¯è¦–åŒ–ã‚’ä½œæˆ"""
        original_rgb = cv2.cvtColor(original, cv2.COLOR_BGR2RGB)
        annotated_rgb = cv2.cvtColor(annotated, cv2.COLOR_BGR2RGB)
        
        fig, axes = plt.subplots(1, 2, figsize=(24, 12))
        
        # å…ƒç”»åƒ
        axes[0].imshow(original_rgb)
        axes[0].set_title('Original test_preview.jpg', fontsize=18, fontweight='bold')
        axes[0].set_xticks([])
        axes[0].set_yticks([])
        
        # å¼·åŒ–æ¤œå‡ºçµæœ
        axes[1].imshow(annotated_rgb)
        axes[1].set_title(f'Enhanced Detection Results\n({len(detections)} lesions detected)', 
                         fontsize=18, fontweight='bold')
        axes[1].set_xticks([])
        axes[1].set_yticks([])
        
        plt.tight_layout()
        plt.savefig("enhanced_test_preview_comparison.png", dpi=300, bbox_inches='tight')
        print("ğŸ’¾ å¼·åŒ–æ¯”è¼ƒå¯è¦–åŒ–ä¿å­˜: enhanced_test_preview_comparison.png")
        plt.show()

def main():
    detector = EnhancedTestPreviewDetector()
    
    print("ğŸš€ test_preview.jpg å¼·åŒ–ç—…å¤‰æ¤œå‡ºé–‹å§‹")
    print("=" * 80)
    
    image_path = "test_preview.jpg"
    detections = detector.multi_scale_detection(image_path)
    
    if detections:
        detector.visualize_results(image_path, detections)
        
        print(f"\nğŸ‰ å¼·åŒ–æ¤œå‡ºå®Œäº†!")
        print(f"ğŸ“Š æ¤œå‡ºã•ã‚ŒãŸç—…å¤‰æ•°: {len(detections)}")
        for i, det in enumerate(detections, 1):
            print(f"  {i}. {det['class_jp']} - ä¿¡é ¼åº¦: {det['confidence']:.3f} ({det['variant']})")
    else:
        print("\nâŒ å¼·åŒ–æ¤œå‡ºã§ã‚‚ç—…å¤‰ã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")

if __name__ == "__main__":
    main()